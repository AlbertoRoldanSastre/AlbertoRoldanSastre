{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlbertoRoldanSastre/AlbertoRoldanSastre/blob/main/efficientdet_tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ITrwSgOieU_"
      },
      "source": [
        "# Efficientdet tensorflow\n",
        "\n",
        "https://github.com/tensorflow/models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuAXpN1EGmO_"
      },
      "source": [
        "# PASO 00. DRIVE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3_7XRmmGqnq",
        "outputId": "1b984431-2c19-4248-cfd4-837f11a46df2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# 1. Montar Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S08FHgTGj6_b"
      },
      "source": [
        "# PASO 0: DOWNLOAD DE DATASET:\n",
        "\n",
        "Opción 1:\n",
        "Descargar en COCO.JSON y pasar a tensorflow TFrecord con el script:\n",
        "\n",
        "Descargar en COCO JSON y luego usar el script create_coco_tfrecord.py (en datasets/ del repo) para convertir a TFRecord.\n",
        "\n",
        "Antes que todo eso, pasas a bounding box, haces tiling, oversamplimg , augmentation y ya con el último COCO.JSON, a TFRecord\n",
        "\n",
        "\n",
        "\n",
        "Repite algo parecido para validación y test, cambiando las rutas.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fWCs9c8w5VM"
      },
      "source": [
        "Intalar un par de cosas antes de transfromar a tfrecord:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9GthWI1kQiA"
      },
      "source": [
        "Opción 2:\n",
        "El repo oficial está pensado para leer TFRecord con las anotaciones embebidas.\n",
        "\n",
        "Roboflow a veces te ofrece descargar directamente en TFRecord. Si ese export funciona bien, genial: ya tienes el dataset listo para “main.py” en EfficientDet.\n",
        "\n",
        "**PROBLEMA**\n",
        "EN TFRecord desde Roboflow solo guarda las bounding boxes.\n",
        "Es mejor seguir todo igual hasta tf record como si fuera pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rzv6KiLH2mKa"
      },
      "source": [
        "# PASO 1: Preparar Entorno y Clonar el Repo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y tensorflow tensorflow-text tensorflow-estimator"
      ],
      "metadata": {
        "id": "wx0skvkUKS1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.15.0 tensorflow-estimator==2.15.0 tensorflow_text==2.15.0 tensorflow-io\n"
      ],
      "metadata": {
        "id": "lu23WVqmKVZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "verificar versiones tras reinciiar:"
      ],
      "metadata": {
        "id": "-GFj7niTKYbj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "empezar"
      ],
      "metadata": {
        "id": "XiLhxsCOKcfl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygsLKWG82pve",
        "outputId": "a76e708b-3515-47a9-e90b-c2b61d60aa1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'models'...\n",
            "remote: Enumerating objects: 4321, done.\u001b[K\n",
            "remote: Counting objects: 100% (4321/4321), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3338/3338), done.\u001b[K\n",
            "remote: Total 4321 (delta 1208), reused 2043 (delta 910), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (4321/4321), 53.31 MiB | 21.92 MiB/s, done.\n",
            "Resolving deltas: 100% (1208/1208), done.\n"
          ]
        }
      ],
      "source": [
        "# Clonar el repo\n",
        "!git clone --depth 1 https://github.com/tensorflow/models.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLpnsICWg8ca",
        "outputId": "e719d5f4-6064-480b-b481-a43f942e6c2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/models/research\n"
          ]
        }
      ],
      "source": [
        "#Compilar los archivos .proto\n",
        "%cd models/research\n",
        "!protoc object_detection/protos/*.proto --python_out=."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8p3odjrCg-4b",
        "outputId": "78ac05a1-947a-4c83-851c-dcfde6dde002"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PYTHONPATH: /content/models/research:/content/models/research/slim:/env/python\n",
            "sys.path: ['/content', '/env/python', '/usr/lib/python311.zip', '/usr/lib/python3.11', '/usr/lib/python3.11/lib-dynload', '', '/usr/local/lib/python3.11/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.11/dist-packages/IPython/extensions', '/root/.ipython', '/content/models/research', '/content/models/research/slim']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# Agrega a PYTHONPATH los directorios necesarios\n",
        "os.environ['PYTHONPATH'] = \"/content/models/research:/content/models/research/slim:\" + os.environ.get('PYTHONPATH', '')\n",
        "sys.path.append('/content/models/research')\n",
        "sys.path.append('/content/models/research/slim')\n",
        "\n",
        "# Opcional: Verifica que las rutas se han añadido\n",
        "print(\"PYTHONPATH:\", os.environ['PYTHONPATH'])\n",
        "print(\"sys.path:\", sys.path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d72MY3Vf18vl",
        "outputId": "d2335101-6310-44b2-9b30-31a7907fa95e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 732K\n",
            "drwxr-xr-x 2 root root 4.0K Mar 17 09:00 anchor_generators\n",
            "drwxr-xr-x 2 root root 4.0K Mar 17 09:00 box_coders\n",
            "drwxr-xr-x 2 root root 4.0K Mar 17 09:00 builders\n",
            "drwxr-xr-x 2 root root 4.0K Mar 17 09:00 colab_tutorials\n",
            "drwxr-xr-x 4 root root 4.0K Mar 17 09:00 configs\n",
            "-rw-r--r-- 1 root root  765 Mar 17 09:00 CONTRIBUTING.md\n",
            "drwxr-xr-x 2 root root 4.0K Mar 17 09:00 core\n",
            "drwxr-xr-x 2 root root 4.0K Mar 17 09:00 data\n",
            "drwxr-xr-x 2 root root 4.0K Mar 17 09:00 data_decoders\n",
            "drwxr-xr-x 4 root root 4.0K Mar 17 09:00 dataset_tools\n",
            "drwxr-xr-x 6 root root 4.0K Mar 17 09:00 dockerfiles\n",
            "-rw-r--r-- 1 root root  55K Mar 17 09:00 eval_util.py\n",
            "-rw-r--r-- 1 root root  20K Mar 17 09:00 eval_util_test.py\n",
            "-rw-r--r-- 1 root root  16K Mar 17 09:00 exporter_lib_tf2_test.py\n",
            "-rw-r--r-- 1 root root  14K Mar 17 09:00 exporter_lib_v2.py\n",
            "-rw-r--r-- 1 root root 7.4K Mar 17 09:00 exporter_main_v2.py\n",
            "-rw-r--r-- 1 root root  28K Mar 17 09:00 exporter.py\n",
            "-rw-r--r-- 1 root root  58K Mar 17 09:00 exporter_tf1_test.py\n",
            "-rw-r--r-- 1 root root 9.4K Mar 17 09:00 export_inference_graph.py\n",
            "-rw-r--r-- 1 root root  17K Mar 17 09:00 export_tflite_graph_lib_tf2.py\n",
            "-rw-r--r-- 1 root root  13K Mar 17 09:00 export_tflite_graph_lib_tf2_test.py\n",
            "-rw-r--r-- 1 root root 6.0K Mar 17 09:00 export_tflite_graph_tf2.py\n",
            "-rw-r--r-- 1 root root  14K Mar 17 09:00 export_tflite_ssd_graph_lib.py\n",
            "-rw-r--r-- 1 root root  20K Mar 17 09:00 export_tflite_ssd_graph_lib_tf1_test.py\n",
            "-rw-r--r-- 1 root root 5.8K Mar 17 09:00 export_tflite_ssd_graph.py\n",
            "drwxr-xr-x 3 root root 4.0K Mar 17 09:00 g3doc\n",
            "drwxr-xr-x 2 root root 4.0K Mar 17 09:00 inference\n",
            "-rw-r--r-- 1 root root    0 Mar 17 09:00 __init__.py\n",
            "-rw-r--r-- 1 root root  54K Mar 17 09:00 inputs.py\n",
            "-rw-r--r-- 1 root root  77K Mar 17 09:00 inputs_test.py\n",
            "drwxr-xr-x 2 root root 4.0K Mar 17 09:00 legacy\n",
            "drwxr-xr-x 2 root root 4.0K Mar 17 09:00 matchers\n",
            "drwxr-xr-x 2 root root 4.0K Mar 17 09:00 meta_architectures\n",
            "drwxr-xr-x 2 root root 4.0K Mar 17 09:00 metrics\n",
            "-rw-r--r-- 1 root root 1.8K Mar 17 09:00 model_hparams.py\n",
            "-rw-r--r-- 1 root root  50K Mar 17 09:00 model_lib.py\n",
            "-rw-r--r-- 1 root root  22K Mar 17 09:00 model_lib_tf1_test.py\n",
            "-rw-r--r-- 1 root root 9.8K Mar 17 09:00 model_lib_tf2_test.py\n",
            "-rw-r--r-- 1 root root  49K Mar 17 09:00 model_lib_v2.py\n",
            "-rw-r--r-- 1 root root 4.5K Mar 17 09:00 model_main.py\n",
            "-rw-r--r-- 1 root root 4.9K Mar 17 09:00 model_main_tf2.py\n",
            "drwxr-xr-x 3 root root  12K Mar 17 09:00 models\n",
            "-rw-r--r-- 1 root root 5.6K Mar 17 09:00 model_tpu_main.py\n",
            "drwxr-xr-x 4 root root 4.0K Mar 17 09:00 packages\n",
            "drwxr-xr-x 3 root root 4.0K Mar 17 09:00 predictors\n",
            "drwxr-xr-x 2 root root 4.0K Mar 17 09:01 protos\n",
            "-rw-r--r-- 1 root root  12K Mar 17 09:00 README.md\n",
            "drwxr-xr-x 4 root root 4.0K Mar 17 09:00 samples\n",
            "drwxr-xr-x 2 root root 4.0K Mar 17 09:00 test_data\n",
            "drwxr-xr-x 4 root root 4.0K Mar 17 09:00 test_images\n",
            "drwxr-xr-x 3 root root 4.0K Mar 17 09:00 tpu_exporters\n",
            "drwxr-xr-x 2 root root 4.0K Mar 17 09:00 utils\n"
          ]
        }
      ],
      "source": [
        "!ls -lh /content/models/research/object_detection"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lvis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTCIMN5z6vzs",
        "outputId": "54f9c4b4-357a-4bab-e4cc-d79d4dc53180"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lvis\n",
            "  Downloading lvis-0.5.3-py3-none-any.whl.metadata (856 bytes)\n",
            "Requirement already satisfied: cycler>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from lvis) (0.12.1)\n",
            "Requirement already satisfied: Cython>=0.29.12 in /usr/local/lib/python3.11/dist-packages (from lvis) (3.0.12)\n",
            "Requirement already satisfied: kiwisolver>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from lvis) (1.4.8)\n",
            "Requirement already satisfied: matplotlib>=3.1.1 in /usr/local/lib/python3.11/dist-packages (from lvis) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.18.2 in /usr/local/lib/python3.11/dist-packages (from lvis) (1.26.4)\n",
            "Requirement already satisfied: opencv-python>=4.1.0.25 in /usr/local/lib/python3.11/dist-packages (from lvis) (4.11.0.86)\n",
            "Requirement already satisfied: pyparsing>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from lvis) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from lvis) (2.8.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from lvis) (1.17.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.1.1->lvis) (1.3.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.1.1->lvis) (4.56.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.1.1->lvis) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.1.1->lvis) (11.1.0)\n",
            "Downloading lvis-0.5.3-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: lvis\n",
            "Successfully installed lvis-0.5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMnrQoMO1kgn"
      },
      "source": [
        "# para convertir json a tfrecord:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10cRQ-o51UpY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78773198-42c7-4f6e-d423-64931c61ecb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contextlib2\n",
            "  Downloading contextlib2-21.6.0-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Downloading contextlib2-21.6.0-py2.py3-none-any.whl (13 kB)\n",
            "Installing collected packages: contextlib2\n",
            "Successfully installed contextlib2-21.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install contextlib2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVQe_s_G1YK4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "outputId": "48346158-9e56-4523-d3ba-fb67351787ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting protobuf==3.20.1\n",
            "  Downloading protobuf-3.20.1-py2.py3-none-any.whl.metadata (720 bytes)\n",
            "Downloading protobuf-3.20.1-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/162.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m153.6/162.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.6\n",
            "    Uninstalling protobuf-4.25.6:\n",
            "      Successfully uninstalled protobuf-4.25.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-cloud-language 2.16.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-pubsub 2.28.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-bigtable 2.29.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-resource-manager 1.14.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-bigquery-connection 1.18.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-aiplatform 1.79.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-translate 3.19.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-datastore 2.20.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.29.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "tensorflow-metadata 1.16.1 requires protobuf<6.0.0dev,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-spanner 3.52.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-dataproc 5.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-firestore 2.20.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "grpcio-status 1.62.3 requires protobuf>=4.21.6, but you have protobuf 3.20.1 which is incompatible.\n",
            "googleapis-common-protos 1.69.1 requires protobuf!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "grpc-google-iam-v1 0.14.1 requires protobuf!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "tensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-api-core 2.24.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-iam 2.18.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\n",
            "google-cloud-functions 1.19.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.20.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-3.20.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "936660c1871e433c8cb96878955ba254"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install protobuf==3.20.3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ste mensaje indica un conflicto de dependencias: algunas librerías (por ejemplo, tensorflow-metadata, grpcio-status y otros paquetes de Google) requieren una versión de protobuf mucho más reciente (≥4.25.2 o ≥4.21.6) para Python 3.11, mientras que para que funcione correctamente la API de detección de TensorFlow (y el código generado a partir de los archivos .proto) se recomienda usar una versión más antigua de protobuf (por ejemplo, 3.20.3).\n",
        "\n",
        "¿Es imprescindible para lo que deseas?\n",
        "Sí e imprescindible:\n",
        "Todas las dependencias deben ser compatibles para que tu entorno de entrenamiento funcione correctamente. Si hay conflictos, podrías experimentar errores en funciones clave o incluso fallos en la ejecución.\n",
        "Opciones para resolverlo:\n",
        "Cambiar la versión de Python en Colab:\n",
        "La forma más robusta es usar un entorno con Python 3.9 o 3.10. En Colab, esto no es trivial (ya que la imagen predeterminada es Python 3.11), pero puedes considerar utilizar un entorno local o buscar una imagen de Colab que use Python 3.10. Con Python 3.9/3.10, podrías instalar versiones de TensorFlow y protobuf compatibles con todos los paquetes.\n",
        "\n",
        "Actualizar todas las dependencias (no recomendable):\n",
        "Intentar forzar el uso de una versión de protobuf más alta (por ejemplo, ≥4.25.2) podría romper la compatibilidad con el código generado por protoc en la API de detección. La API está escrita y probada con versiones antiguas de protobuf, por lo que actualizar puede generar errores difíciles de resolver.\n",
        "\n",
        "Usar un entorno virtual distinto:\n",
        "Si tienes la posibilidad de configurar un entorno (por ejemplo, localmente o en un contenedor) con Python 3.9, podrías instalar TensorFlow 2.15 o 2.18 junto con la versión de protobuf requerida (por ejemplo, 3.20.3 o versiones compatibles) sin conflictos con tensorflow-metadata y demás.\n",
        "\n",
        "Recomendación:\n",
        "Para el desarrollo de tu tesis y para que el entorno sea estable, te recomiendo usar un entorno con Python 3.9 o 3.10. Esto permitirá que instales versiones de TensorFlow y las dependencias (incluyendo tensorflow-metadata, grpcio-status, etc.) que sean compatibles entre sí. En Colab, esto puede ser complicado, por lo que si decides usar Colab, quizás debas considerar ejecutar el entrenamiento en un entorno local o en una máquina virtual configurada con una versión de Python compatible.\n",
        "\n",
        "Resumen:\n",
        "El conflicto surge porque algunas librerías requieren una versión de protobuf ≥4.25.2, mientras que la API de detección de TensorFlow (y el código generado) es compatible con una versión antigua (3.20.3).\n",
        "Es esencial resolver estos conflictos para que tu modelo se entrene de forma estable.\n",
        "La solución más robusta es usar un entorno con Python 3.9/3.10, en el cual puedas instalar versiones de todas las dependencias sin conflicto.\n",
        "Si no puedes cambiar la versión de Python en Colab, es posible que tengas que buscar una solución intermedia, pero ten en cuenta que forzar una versión más reciente de protobuf puede romper la compatibilidad con la API de detección.\n",
        "Espero que esta explicación te ayude a entender el conflicto y te oriente sobre la mejor manera de resolverlo para tu proyecto. Si tienes más dudas o necesitas más detalles, ¡aquí estoy para ayudarte!"
      ],
      "metadata": {
        "id": "jQxmp_CbjiRI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQsJ9jD_1bJD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de897383-4fda-467d-b8a1-6951eeb19190"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.20.1\n"
          ]
        }
      ],
      "source": [
        "import google.protobuf\n",
        "print(google.protobuf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ro4BVM0PmPmr"
      },
      "outputs": [],
      "source": [
        "!python models/research/object_detection/dataset_tools/create_coco_tf_record.py \\\n",
        "    --logtostderr \\\n",
        "    --train_image_dir=\"/content/drive/MyDrive/prueba_efficient/prueba/train\" \\\n",
        "    --val_image_dir=\"/content/drive/MyDrive/prueba_efficient/prueba/valid\" \\\n",
        "    --test_image_dir=\"/content/drive/MyDrive/prueba_efficient/prueba/test\" \\\n",
        "    --train_annotations_file=\"/content/drive/MyDrive/prueba_efficient/prueba/annotations/instances_train.json\" \\\n",
        "    --val_annotations_file=\"/content/drive/MyDrive/prueba_efficient/prueba/annotations/instances_valid.json\" \\\n",
        "    --testdev_annotations_file=\"/content/drive/MyDrive/prueba_efficient/prueba/annotations/instances_test.json\" \\\n",
        "    --output_dir=\"/content/COCO_TFRecords\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define la carpeta de destino en Google Drive\n",
        "dest_folder = \"/content/drive/MyDrive/tensorflow_prueba\"\n",
        "!mkdir -p \"{dest_folder}\"                                                           #El comando mkdir -p crea esa carpeta (y cualquier subcarpeta necesaria) si no existe.\n",
        "\n",
        "# Copia la carpeta de resultados (ajusta el nombre según el que se haya generado)\n",
        "!cp -r /content/COCO_TFRecords \"{dest_folder}/\"   #copiar los reultados del modelo en drive\n",
        "\n",
        "\n",
        "# Comprueba que se copiaron los archivos (opcional)\n",
        "!ls \"{dest_folder}\"                                                                 #verifica que los archivos se han copiado correctamente\n",
        "\n",
        "# Opcional: comprimir la carpeta de resultados para descargarla a tu PC\n",
        "#!zip -r /content/saved/prueba.zip \"{dest_folder}/efficient_prueba\"   #Aquí se comprime la carpeta que acabas de copiar en Drive (la subcarpeta focas_training_prueba1_640_24_02_25 dentro de foquitas) en un archivo ZIP llamado foquitas.zip que se guarda en /content (la carpeta principal del entorno de Colab).\n",
        "\n",
        "# Descargar el archivo ZIP a tu PC\n",
        "#from google.colab import files\n",
        "#files.download('/content/inferencia_2.zip')"
      ],
      "metadata": {
        "id": "HYOK2wsOr8Yc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "buscar archivo:\n",
        "en este caso , el d1 que tienen entrada imagenes 640, pero hay más\n",
        "buscar en carpeta\n",
        "\n",
        "/content/models/research/object_detection/configs/tf2/"
      ],
      "metadata": {
        "id": "YcP3n1138BnJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!find /content/models/research/object_detection/configs/tf2/ -type f -name ssd_efficientdet_d1_640x640_coco17_tpu-8.config"
      ],
      "metadata": {
        "id": "1AEm22Jx8AuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Antes de entrenar debemos cambiar esos archivos de la siguiente forma:\n",
        "\n",
        "los input_paths son las rutas a donde estan los tf record, poniendo al final el nombre gneral del archivo que detecta train, o valid, en su parte correspondiente\n",
        "\n",
        "y label_map es el archivo pbtxt donde pones todas tus clases , hay un ejemplo en carpeta tensorflow mia"
      ],
      "metadata": {
        "id": "2GLyw1RKL-M8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_input_reader {\n",
        "  input_path: \"/content/drive/MyDrive/tensorflow_prueba/COCO_TFRecords/coco_train.record\"\n",
        "  label_map_path: \"/content/drive/MyDrive/tensorflow_prueba/label_map.pbtxt\"\n",
        "  shuffle: true\n",
        "  num_readers: 1\n",
        "}\n",
        "\n",
        "eval_input_reader {\n",
        "  input_path: \"/content/drive/MyDrive/tensorflow_prueba/COCO_TFRecords/coco_val.record\"\n",
        "  label_map_path: \"/content/drive/MyDrive/tensorflow_prueba/label_map.pbtxt\"\n",
        "  shuffle: false\n",
        "  num_readers: 1\n",
        "}"
      ],
      "metadata": {
        "id": "OM_LVHQLMRIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dPaBozS32-N"
      },
      "source": [
        "# Train:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "el mode_dir= es donde se guarda lo que haces"
      ],
      "metadata": {
        "id": "7ia6yIUde47F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**para cambiar parámetros:**\n",
        "en el archivo de configuracion:\n",
        "\n"
      ],
      "metadata": {
        "id": "19Zgor-nyyOM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En el bloque optimizer del train_config se define la tasa de aprendizaje (learning_rate) y el momentum. Por ejemplo"
      ],
      "metadata": {
        "id": "zoZecdh1y2k-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si quieres ajustar la tasa de aprendizaje, cambia learning_rate_base y/o warmup_learning_rate.\n",
        "Si deseas modificar el momentum, cambia momentum_optimizer_value."
      ],
      "metadata": {
        "id": "dWK_e4ThzIOr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Número de clases (num_classes):\n",
        "Aunque tengas un archivo label_map.pbtxt que mapea los IDs a nombres, el modelo necesita saber cuántas salidas generar en su capa de clasificación. En el bloque model { ssd { ... } } busca la línea:\n",
        "cambialo al numero de clases tuya"
      ],
      "metadata": {
        "id": "SZZ0LwxUzMZq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "calcular las epochs:\n",
        "\n",
        "num_steps = (número de imágenes de entrenamiento / batch_size) * epochs"
      ],
      "metadata": {
        "id": "wgLXHfMLyQP6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "para navegar al directorio y correr entrenamiento\n"
      ],
      "metadata": {
        "id": "l9ProdGRu_2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/models/research/object_detection"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjomySO3u-qs",
        "outputId": "aca17fad-de18-45bb-e912-f345522532fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/models/research/object_detection\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URNJqrJH4aYN",
        "outputId": "ec0ad046-49c6-4f34-b884-812641c91bfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/models/research/object_detection\n",
            "2025-03-17 09:07:42.677970: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1742202462.710410    2819 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1742202462.720192    2819 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-17 09:07:42.765182: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/models/research/object_detection/model_main_tf2.py\", line 31, in <module>\n",
            "    from object_detection import model_lib_v2\n",
            "ModuleNotFoundError: No module named 'object_detection'\n"
          ]
        }
      ],
      "source": [
        "%cd /content/models/research/object_detection\n",
        "!python model_main_tf2.py \\\n",
        "    --pipeline_config_path=/content/models/research/object_detection/configs/tf2/ssd_efficientdet_d1_640x640_coco17_tpu-8.config \\\n",
        "    --model_dir=/content/efficientdet_model \\\n",
        "    --alsologtostderr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFtz5hNf4sAo"
      },
      "source": [
        "# validar:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "syWXrnJW4ycw"
      },
      "outputs": [],
      "source": [
        "!python main.py \\\n",
        "  --mode=eval \\\n",
        "  --validation_file_pattern=\"/content/dataset/tf_records/val-*.tfrecord\" \\\n",
        "  --model_name=\"efficientdet-d0\" \\\n",
        "  --model_dir=\"/content/model_dir/d0\" \\\n",
        "  --eval_samples=2000\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DglRiBYS41g3"
      },
      "source": [
        "Inferencia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51Fsy7uw478k"
      },
      "outputs": [],
      "source": [
        "!python model_inspect.py \\\n",
        "  --mode=export \\\n",
        "  --model_name=\"efficientdet-d0\" \\\n",
        "  --model_dir=\"/content/model_dir/d0\" \\\n",
        "  --saved_model_dir=\"/content/savedmodel\" \\\n",
        "  --tflite_path=\"/content/model.tflite\" \\\n",
        "  --export_format=\"tflite\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PROBANDO COSAS NUEVAS\n"
      ],
      "metadata": {
        "id": "L5_b1JHBBN61"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNBKMuhr2QbHOalqJgkIBAG",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}